{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96a75c68-86e7-4d92-9908-f5f5d485de6e",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7b4676-e428-4ec4-b851-e26a33947ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used Functions:\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "# Developed Functions:\n",
    "from Dev_Fun.Silhouette_Extraction import makeSegMask\n",
    "from Dev_Fun.Joints_Extraction import mediapipe_detection, draw_landmarks\n",
    "from Dev_Fun.Calibration_Show import Calib_Show\n",
    "from Dev_Fun.Dataset_Capture import files_creation, dataset_capture\n",
    "from Dev_Fun.Cropping_Image import crop_to_person\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de555518",
   "metadata": {},
   "source": [
    "# Calibration\n",
    "<img src=\"https://www.researchgate.net/profile/Jungpil-Shin/publication/362351225/figure/fig3/AS:1183521695645696@1659184969422/Mediapipe-detects-33-nodes-of-the-human-pose.ppm\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff11962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed Input Source\n",
    "feed = 0\n",
    "# Calib_Show(feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b6ed6c",
   "metadata": {},
   "source": [
    "# Setup Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('Custom_Gait_Dataset_V.4')\n",
    "EXPORTED = os.path.join('Exported')\n",
    "\n",
    "# Persons that we try to detect\n",
    "persons = np.array(['Lance Moheb','Seif Ibrahim','Mina Nady','Omar Amin','Mohmed Youssef','Hamza Tharwat','Romario Nagy','Ziad Diaa'])\n",
    "\n",
    "# 10 videos worth of data\n",
    "no_sequences = 10\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 90\n",
    "\n",
    "persons_len = len(persons)\n",
    "total_sequences = persons_len*no_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e9061",
   "metadata": {},
   "source": [
    "# Capturing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_creation(persons,no_sequences,DATA_PATH)\n",
    "# dataset_capture(persons,no_sequences,DATA_PATH,sequence_length,feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae804103",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a7c073",
   "metadata": {},
   "source": [
    "* A) Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b3be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_map = {label:num for num, label in enumerate(persons)}\n",
    "# print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22cd3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences, labels = [], []\n",
    "# for person in persons:\n",
    "#     for sequence in np.array(os.listdir(os.path.join(DATA_PATH, person))).astype(int):\n",
    "#         window = []\n",
    "#         for frame_num in range(sequence_length):\n",
    "\n",
    "#             image_path = os.path.join(DATA_PATH, person, str(sequence), f\"{frame_num}.jpg\")\n",
    "#             print (image_path)\n",
    "#             res = cv2.imread(image_path)\n",
    "#             window.append(res)\n",
    "#         sequences.append(window)\n",
    "#         labels.append(label_map[person])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c625fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seqq=np.array(sequences)\n",
    "# print(seqq.shape)\n",
    "# show_test = cv2.cvtColor(seqq[0][30], cv2.COLOR_BGR2RGB)\n",
    "# plt.imshow(show_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2fd482",
   "metadata": {},
   "source": [
    "* B) dataset preproccesing\n",
    "    * Flip Horizontal\n",
    "    * Converting to Silhouette\n",
    "        * Background Subtraction\n",
    "        * Noise Reduction\n",
    "        * Binarization\n",
    "    * convert to gray-scale\n",
    "    * Resize to CASIA-B Resolution (320x240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f9580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proccessed = []\n",
    "# fgbg = cv2.createBackgroundSubtractorKNN(detectShadows=False)\n",
    "# for sequence in  range(total_sequences):\n",
    "# # for sequence in  range(1):\n",
    "#     for frame_num in range(sequence_length):\n",
    "\n",
    "#         frame = cv2.cvtColor(seqq[sequence][frame_num], cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#         flip = cv2.flip(frame, 1)\n",
    "\n",
    "#         sil = makeSegMask(flip)\n",
    "\n",
    "#         gray = cv2.cvtColor(sil, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#         resize = cv2.resize(gray,(320,240))\n",
    "\n",
    "#         crop = crop_to_person(resize)\n",
    "\n",
    "#         proccessed.append(crop)\n",
    "        \n",
    "#         print (len(proccessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927d8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(proccessed[30],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf696b5",
   "metadata": {},
   "source": [
    "* C) Export the Custom dataset with CASIA-B File Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ad9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_directories_if_not_exist(path):\n",
    "#     if not os.path.exists(path):\n",
    "#         os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e268f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your dataset path\n",
    "# your_dataset_path = \"CASIA-B\"\n",
    "\n",
    "# # Define walking conditions\n",
    "# walking_conditions = [\n",
    "#     \"bg-01\", \"bg-02\",\n",
    "#     \"cl-01\", \"cl-02\",\n",
    "#     \"nm-01\", \"nm-02\", \"nm-03\", \"nm-04\", \"nm-05\", \"nm-06\"\n",
    "# ]\n",
    "\n",
    "# # Iterate over subjects\n",
    "# for subject_id in range(119, 128):\n",
    "#     subject_folder = os.path.join(your_dataset_path, str(subject_id))\n",
    "\n",
    "#     # Iterate over walking conditions\n",
    "#     for walking_condition in walking_conditions:\n",
    "#         walking_condition_folder = os.path.join(subject_folder, walking_condition,str(\"090\"))\n",
    "\n",
    "#         # Iterate over frames\n",
    "#         for frame_num in range(90):\n",
    "#             # Define frame name\n",
    "#             frame_name = f\"{subject_id}-{walking_condition}-090-{frame_num:03d}.png\"\n",
    "#             frame_path = os.path.join(walking_condition_folder, frame_name)\n",
    "\n",
    "#             walking_condition_index = walking_conditions.index(walking_condition)\n",
    "#             index = (subject_id - 119) * len(walking_conditions) * 90 + walking_conditions.index(walking_condition) * 90 + frame_num\n",
    "#             print(index)\n",
    "\n",
    "#             # Check if the image is empty\n",
    "#             if proccessed[index] is None:\n",
    "#                 # Create a white image\n",
    "#                 image = np.ones((128, 64, 3), dtype=np.uint8) * 255  # Assuming 128x64 is the size of your image\n",
    "#             else:\n",
    "#                 image = proccessed[index]\n",
    "\n",
    "#             create_directories_if_not_exist(walking_condition_folder)\n",
    "#             cv2.imwrite(frame_path, image)\n",
    "#             print(f\"Exported: {frame_path}\")\n",
    "\n",
    "# print(\"Images exported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d69a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset path\n",
    "your_dataset_path = \"CASIA-B\"\n",
    "\n",
    "# Define walking conditions\n",
    "walking_conditions = [\n",
    "    \"bg-01\", \"bg-02\",\n",
    "    \"cl-01\", \"cl-02\",\n",
    "    \"nm-01\", \"nm-02\", \"nm-03\", \"nm-04\", \"nm-05\", \"nm-06\"\n",
    "]\n",
    "\n",
    "# Iterate over subjects\n",
    "for subject_id in range(119):\n",
    "    subject_folder = os.path.join(your_dataset_path, str(subject_id))\n",
    "\n",
    "    # Iterate over walking conditions\n",
    "    for walking_condition in walking_conditions:\n",
    "        walking_condition_folder = os.path.join(subject_folder, walking_condition,str(\"090\"))\n",
    "\n",
    "        # Iterate over frames\n",
    "        for frame_num in range(90):\n",
    "            # Define frame name\n",
    "            frame_name = f\"{subject_id}-{walking_condition}-090-{frame_num:03d}.png\"\n",
    "            frame_path = os.path.join(walking_condition_folder, frame_name)\n",
    "\n",
    "            walking_condition_index = walking_conditions.index(walking_condition)\n",
    "            index = (subject_id - 119) * len(walking_conditions) * 90 + walking_conditions.index(walking_condition) * 90 + frame_num\n",
    "            print(index)\n",
    "\n",
    "            image = proccessed[index]\n",
    "\n",
    "            create_directories_if_not_exist(walking_condition_folder)\n",
    "            cv2.imwrite(frame_path, image)\n",
    "            print(f\"Exported: {frame_path}\")\n",
    "\n",
    "print(\"Images exported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc825f3",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b34087",
   "metadata": {},
   "source": [
    "A) Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceda2494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array(sequences)\n",
    "# y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9427970c",
   "metadata": {},
   "source": [
    "B) Building CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dir = os.path.join('Logs')\n",
    "# tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "# from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb7782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(60,132)))\n",
    "# model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "# model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(persons.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b631369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Compilation\n",
    "# model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, epochs=200, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8028237",
   "metadata": {},
   "source": [
    "C) Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a203c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aae120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Evaluate on test data\")\n",
    "# results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "# print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b088dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bccae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persons[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111484ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persons[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee87f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('train4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4268cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2d91c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "# yhat = np.argmax(yhat, axis=1).tolist()\n",
    "\n",
    "# multilabel_confusion_matrix(ytrue, yhat)\n",
    "\n",
    "# accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803529ec",
   "metadata": {},
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cbd420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = [(245,117,16),(245,117,16),(245,117,16),(245,117,16),(245,117,16),(245,117,16),(245,117,16),(245,117,16),(245,117,16),(245,117,16),(245,117,16),(245,117,16)]\n",
    "# def prob_viz(res, actions, input_frame, colors):\n",
    "#     output_frame = input_frame.copy()\n",
    "#     for num, prob in enumerate(res):\n",
    "#         cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "#         cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 1, cv2.LINE_AA)\n",
    "        \n",
    "#     return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3639b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. New detection variables\n",
    "# sequence = []\n",
    "# sentence = []\n",
    "# predictions = []\n",
    "# threshold = 0.3\n",
    "\n",
    "# cap = cv2.VideoCapture(feed)\n",
    "# # Set mediapipe model\n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#     while cap.isOpened():\n",
    "\n",
    "#         # Read feed\n",
    "#         ret, frame = cap.read()\n",
    "\n",
    "#         # Make detections\n",
    "#         image, results = mediapipe_detection(frame, holistic)\n",
    "#         print(results)\n",
    "        \n",
    "#         # Draw landmarks\n",
    "#         draw_landmarks(image, results)\n",
    "        \n",
    "#         # 2. Prediction logic\n",
    "#         keypoints = extract_keypoints(results)\n",
    "#         sequence.append(keypoints)\n",
    "#         sequence = sequence[-60:]\n",
    "        \n",
    "#         if len(sequence) == 60:\n",
    "#             res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "#             print(persons[np.argmax(res)])\n",
    "#             predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "#         #3. Viz logic\n",
    "#             if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "#                 if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "#                     if len(sentence) > 0: \n",
    "#                         if persons[np.argmax(res)] != sentence[-1]:\n",
    "#                             sentence.append(persons[np.argmax(res)])\n",
    "#                     else:\n",
    "#                         sentence.append(persons[np.argmax(res)])\n",
    "\n",
    "#             if len(sentence) > 5: \n",
    "#                 sentence = sentence[-5:]\n",
    "\n",
    "#             # Viz probabilities\n",
    "#             image = prob_viz(res, persons, image, colors)\n",
    "            \n",
    "#         cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "                \n",
    "#         cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "#                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        \n",
    "#         # Show to screen\n",
    "#         cv2.imshow('Gait ID', image)\n",
    "\n",
    "#         # Break gracefully\n",
    "#         if cv2.waitKey(10) & 0xFF == 27:\n",
    "#             break\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
