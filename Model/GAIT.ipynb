{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96a75c68-86e7-4d92-9908-f5f5d485de6e",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7b4676-e428-4ec4-b851-e26a33947ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main Functions:\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# from tensorflow.keras import layers\n",
    "# from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# #  Functions:\n",
    "# import os\n",
    "# from glob import glob\n",
    "# from matplotlib import pyplot as plt\n",
    "# from collections import Counter\n",
    "\n",
    "# # Developed Functions: \n",
    "# from Dev_Fun.Calibration_Show import Calib_Show, mediapipe_detection, draw_landmarks\n",
    "# from Dev_Fun.Dataset_Capture import files_creation, dataset_capture\n",
    "# from Dev_Fun.Pre_Process import pre_process, crop_to_person, sil_V1, sil_V2\n",
    "# from Dev_Fun.Predict import pred_show, final_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de555518",
   "metadata": {},
   "source": [
    "# Calibration\n",
    "<img src=\"https://www.researchgate.net/profile/Jungpil-Shin/publication/362351225/figure/fig3/AS:1183521695645696@1659184969422/Mediapipe-detects-33-nodes-of-the-human-pose.ppm\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff11962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed Input Source\n",
    "feed = 0\n",
    "# Calib_Show(feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b6ed6c",
   "metadata": {},
   "source": [
    "# Setup Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('Custom_Gait_Dataset_V.4')\n",
    "EXPORTED = os.path.join('Exported')\n",
    "\n",
    "# Persons that we try to detect\n",
    "persons = np.array(['Lance Moheb','Seif Ibrahim','Mina Nady','Omar Amin','Mohmed Youssef','Hamza Tharwat'])\n",
    "# persons = np.array(['Lance Moheb','Seif Ibrahim','Mina Nady','Omar Amin','Mohmed Youssef','Hamza Tharwat','Romario Nagy','Ziad Diaa'])\n",
    "\n",
    "# 10 videos worth of data\n",
    "no_sequences = 10\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 90\n",
    "\n",
    "persons_len = len(persons)\n",
    "total_sequences = persons_len*no_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e9061",
   "metadata": {},
   "source": [
    "# Capturing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_creation(persons,no_sequences,DATA_PATH)\n",
    "# dataset_capture(persons,no_sequences,DATA_PATH,sequence_length,feed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae804103",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a7c073",
   "metadata": {},
   "source": [
    "* A) Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b3be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_map = {label:num for num, label in enumerate(persons)}\n",
    "# print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22cd3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences, labels = [], []\n",
    "# for person in persons:\n",
    "#     for sequence in np.array(os.listdir(os.path.join(DATA_PATH, person))).astype(int):\n",
    "#         window = []\n",
    "#         for frame_num in range(sequence_length):\n",
    "\n",
    "#             image_path = os.path.join(DATA_PATH, person, str(sequence), f\"{frame_num}.jpg\")\n",
    "#             print (image_path)\n",
    "#             res = cv2.imread(image_path)\n",
    "#             window.append(res)\n",
    "#         sequences.append(window)\n",
    "#         labels.append(label_map[person])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c625fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seqq=np.array(sequences)\n",
    "# print(seqq.shape)\n",
    "# show_test = cv2.cvtColor(seqq[0][30], cv2.COLOR_BGR2RGB)\n",
    "# plt.imshow(show_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2fd482",
   "metadata": {},
   "source": [
    "* B) dataset preproccesing\n",
    "    * Flip Horizontal\n",
    "    * Converting to Silhouette\n",
    "        * Background Subtraction\n",
    "        * Noise Reduction\n",
    "        * Binarization\n",
    "    * convert to gray-scale\n",
    "    * Resize to CASIA-B Resolution (320x240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f9580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proccessed = []\n",
    "# fgbg = cv2.createBackgroundSubtractorKNN(detectShadows=False)\n",
    "# for sequence in  range(total_sequences):\n",
    "# # for sequence in  range(1):\n",
    "#     for frame_num in range(sequence_length):\n",
    "\n",
    "#         frame = cv2.cvtColor(seqq[sequence][frame_num], cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#         flip = cv2.flip(frame, 1)\n",
    "\n",
    "#         sil = sil_V2(flip)\n",
    "\n",
    "#         gray = cv2.cvtColor(sil, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#         resize = cv2.resize(gray,(320,240))\n",
    "\n",
    "#         crop = crop_to_person(resize)\n",
    "\n",
    "#         proccessed.append(crop)\n",
    "        \n",
    "#         print (len(proccessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927d8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(proccessed[30],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf696b5",
   "metadata": {},
   "source": [
    "* C) Export the Custom dataset with CASIA-B File Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ad9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_directories_if_not_exist(path):\n",
    "#     if not os.path.exists(path):\n",
    "#         os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e268f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your dataset path\n",
    "# dataset_path = \"CASIA-B\"\n",
    "\n",
    "# # Define walking conditions\n",
    "# walking_conditions = [\n",
    "#     \"bg-01\", \"bg-02\",\n",
    "#     \"cl-01\", \"cl-02\",\n",
    "#     \"nm-01\", \"nm-02\", \"nm-03\", \"nm-04\", \"nm-05\", \"nm-06\"\n",
    "# ]\n",
    "\n",
    "# # Iterate over subjects\n",
    "# for subject_id in range(119, 128):\n",
    "#     subject_folder = os.path.join(dataset_path, str(subject_id))\n",
    "\n",
    "#     # Iterate over walking conditions\n",
    "#     for walking_condition in walking_conditions:\n",
    "#         walking_condition_folder = os.path.join(subject_folder, walking_condition,str(\"090\"))\n",
    "\n",
    "#         # Iterate over frames\n",
    "#         for frame_num in range(90):\n",
    "#             # Define frame name\n",
    "#             frame_name = f\"{subject_id}-{walking_condition}-090-{frame_num:03d}.png\"\n",
    "#             frame_path = os.path.join(walking_condition_folder, frame_name)\n",
    "\n",
    "#             walking_condition_index = walking_conditions.index(walking_condition)\n",
    "#             index = (subject_id - 119) * len(walking_conditions) * 90 + walking_conditions.index(walking_condition) * 90 + frame_num\n",
    "#             print(index)\n",
    "\n",
    "#             # Check if the image is empty\n",
    "#             if proccessed[index] is None:\n",
    "#                 # Create a white image\n",
    "#                 image = np.ones((128, 64, 3), dtype=np.uint8) * 255  # Assuming 128x64 is the size of your image\n",
    "#             else:\n",
    "#                 image = proccessed[index]\n",
    "\n",
    "#             create_directories_if_not_exist(walking_condition_folder)\n",
    "#             cv2.imwrite(frame_path, image)\n",
    "#             print(f\"Exported: {frame_path}\")\n",
    "\n",
    "# print(\"Images exported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad8c7d",
   "metadata": {},
   "source": [
    "* D) Pre-Proccesing CASIA-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0081f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_images(input):\n",
    "  image_pattern = os.path.join(input, \"*.png\")\n",
    "  image_paths = glob(image_pattern)\n",
    "\n",
    "  return image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d69a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your dataset path\n",
    "# dataset_path = \"CASIA-B\"\n",
    "# export_path = \"exported\"\n",
    "\n",
    "# # Define walking conditions\n",
    "# walking_conditions = [\n",
    "#     \"bg-01\", \"bg-02\",\n",
    "#     \"cl-01\", \"cl-02\",\n",
    "#     \"nm-01\", \"nm-02\", \"nm-03\", \"nm-04\", \"nm-05\", \"nm-06\"\n",
    "# ]\n",
    "\n",
    "# # Iterate over subjects\n",
    "# for subject_id in range(1,119):\n",
    "#     subject_folder = os.path.join(dataset_path, f\"{subject_id:03d}\")\n",
    "#     subject_folder_export = os.path.join(export_path, f\"{subject_id:03d}\")\n",
    "\n",
    "#     # Iterate over walking conditions\n",
    "#     for walking_condition in walking_conditions:\n",
    "#         walking_condition_folder = os.path.join(subject_folder, walking_condition,str(\"090\"))\n",
    "#         walking_condition_folder_export = os.path.join(subject_folder_export, walking_condition,str(\"090\"))\n",
    "#         create_directories_if_not_exist(walking_condition_folder_export)\n",
    "        \n",
    "#         image_list = import_images(walking_condition_folder)\n",
    "#         for image_path in image_list:\n",
    "\n",
    "#             img = cv2.imread(image_path)\n",
    "#             gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#             crop = crop_to_person(gray)\n",
    "\n",
    "#             new_path = image_path.replace(\"CASIA-B\", \"exported\")\n",
    "\n",
    "#             if crop is None:\n",
    "#                 break\n",
    "#             else:\n",
    "#                 cv2.imwrite(new_path, crop)\n",
    "\n",
    "#             print(new_path, \"###DONE###\")\n",
    "# print(\"Images exported successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc825f3",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b34087",
   "metadata": {},
   "source": [
    "A) Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba8393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjects = np.array([f\"{i:03d}\" for i in range(1, 125)])\n",
    "# # subjects = np.concatenate((numbers_array, persons))\n",
    "# print(len(subjects))\n",
    "# print(subjects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baa2a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_map = {label:num for num, label in enumerate(subjects)}\n",
    "# print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac84c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define your dataset path\n",
    "# dataset_path = \"Custom_Gait_Dataset_V.5\"\n",
    "# sequneces_train, sequneces_test, labels_train, labels_test  = [], [], [], []\n",
    "\n",
    "# # Define walking conditions\n",
    "# walking_conditions_train = [\n",
    "#     \"nm-01\", \"nm-02\", \"nm-03\", \"nm-04\"\n",
    "# ]\n",
    "# walking_conditions_test = [\n",
    "#     \"nm-05\", \"nm-06\"\n",
    "# ]\n",
    "\n",
    "# error_num = 0\n",
    "# error = []\n",
    "\n",
    "\n",
    "# # ---Taining Set---\n",
    "# for subject_id in subjects:\n",
    "#     subject_folder = os.path.join(dataset_path, subject_id)\n",
    "\n",
    "#     set = []\n",
    "#     # Iterate over walking conditions\n",
    "#     for walking_condition in walking_conditions_train:\n",
    "#         walking_condition_folder = os.path.join(subject_folder, walking_condition,str(\"090\"))\n",
    "        \n",
    "#         window = []\n",
    "\n",
    "#         image_list = import_images(walking_condition_folder)\n",
    "#         for image_path in image_list[:30]:\n",
    "#             img = cv2.imread(image_path)\n",
    "#             gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#             print(image_path)\n",
    "#             window.append(gray)\n",
    "#         print(\"------------------------------------------\")\n",
    "#         # set.append(window)\n",
    "#         labels_train.append(label_map[str(subject_id)])\n",
    "#         sequneces_train.append(window)\n",
    "\n",
    "#         if(len(window)!= 30):\n",
    "#             error_num += 1\n",
    "#             error.append(subject_id)\n",
    "#     print(\"#################################################################\")\n",
    "    \n",
    "\n",
    "# # ---Testing Set---\n",
    "# for subject_id in subjects:\n",
    "#     subject_folder = os.path.join(dataset_path, subject_id)\n",
    "\n",
    "#     set = []\n",
    "#     # Iterate over walking conditions\n",
    "#     for walking_condition in walking_conditions_test:\n",
    "#         walking_condition_folder = os.path.join(subject_folder, walking_condition,str(\"090\"))\n",
    "        \n",
    "#         window = []\n",
    "\n",
    "#         image_list = import_images(walking_condition_folder)\n",
    "#         for image_path in image_list[:30]:\n",
    "#             img = cv2.imread(image_path)\n",
    "#             gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#             print(image_path)\n",
    "#             window.append(gray)\n",
    "#         print(\"------------------------------------------\")\n",
    "#         # set.append(window)\n",
    "#         labels_test.append(label_map[str(subject_id)])\n",
    "#         sequneces_test.append(window)\n",
    "\n",
    "#         if(len(window)!= 30):\n",
    "#             error_num += 1\n",
    "#             error.append(subject_id)\n",
    "#     print(\"#################################################################\")\n",
    "    \n",
    "# print(\"Images loaded successfully!\")\n",
    "# print(\"errors \" + str(error_num))\n",
    "# print(\"error list:  \" + str(error))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e709c08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.array(sequneces_train)\n",
    "# x_test = np.array(sequneces_test)\n",
    "# y_train = to_categorical(labels_train).astype(int)\n",
    "# y_test = to_categorical(labels_test).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a905b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train.shape)\n",
    "# print(x_test.shape)\n",
    "# print(y_train.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9427970c",
   "metadata": {},
   "source": [
    "B) Building CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dir = os.path.join('Logs')\n",
    "# tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342d552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cnn_model():\n",
    "#   model = keras.Sequential([\n",
    "#     # First convolutional block\n",
    "#     layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(30, 64, 44)),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "#     # Second convolutional block\n",
    "#     layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "#     # Third convolutional block\n",
    "#     layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "#     # Flatten the output of the convolutional layers\n",
    "#     layers.Flatten(),\n",
    "\n",
    "#     # Fully connected layers\n",
    "#     layers.Dense(128, activation=\"relu\"),\n",
    "#     layers.Dense(256, activation=\"relu\"),\n",
    "\n",
    "#     # Output layer with the number of classes (modify this based on your dataset)\n",
    "#     layers.Dense(124, activation=\"softmax\")\n",
    "#   ])\n",
    "\n",
    "#   # Compile the model\n",
    "#   model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "#   return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb7782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bceec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x_train, y_train, batch_size=32 , epochs=20, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8028237",
   "metadata": {},
   "source": [
    "C) Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a203c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aae120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Evaluate on test data\")\n",
    "# results = model.evaluate(x_test, y_test)\n",
    "# print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b088dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = model.predict(x_test)\n",
    "# index = 2\n",
    "# tst = subjects[np.argmax(res[index])]\n",
    "# trn = subjects[np.argmax(y_test[index])]\n",
    "# print(\"Subject tested:\" + tst + \"\\n\" + \"Subject trained:\" + trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee87f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('train1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2d91c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yhat = model.predict(x_test)\n",
    "# ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "# yhat = np.argmax(yhat, axis=1).tolist()\n",
    "\n",
    "# print(f\"\\n ##########< Accuracy: {(accuracy_score(ytrue, yhat)*100):.2f}% >########## \\n\")\n",
    "\n",
    "# print(multilabel_confusion_matrix(ytrue, yhat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803529ec",
   "metadata": {},
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e9e35e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\lance/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n",
      "c:\\Users\\lance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DeepLabV3_ResNet101_Weights.COCO_WITH_VOC_LABELS_V1`. You can also use `weights=DeepLabV3_ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\lance/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been loaded.\n"
     ]
    }
   ],
   "source": [
    "from Dev_Fun.Predict import final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "244747db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\lance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\lance\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "not pushed\n",
      "not pushed\n",
      "not pushed\n",
      "not pushed\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "Seif Ibrahim\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Mina Nady\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Mina Nady\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Mina Nady\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Mina Nady\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Mina Nady\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Mina Nady\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Mina Nady\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Mina Nady\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Mina Nady\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Mina Nady\n",
      "End of video.\n",
      "predictions array: ['Seif Ibrahim', 'Mina Nady', 'Mina Nady', 'Mina Nady', 'Mina Nady', 'Mina Nady', 'Mina Nady', 'Mina Nady', 'Mina Nady', 'Mina Nady', 'Mina Nady']\n",
      "The most repeated element is: Mina Nady \n",
      "Percentage:  90.9090909090909\n",
      "Mina Nady\n",
      "Mina Nady\n"
     ]
    }
   ],
   "source": [
    "name = final_pred(\"Test_Samples/test4.mp4\",\"Saved_Models/train_55_6.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6ccdb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mina Nady\n"
     ]
    }
   ],
   "source": [
    "print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
