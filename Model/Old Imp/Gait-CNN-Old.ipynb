{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96a75c68-86e7-4d92-9908-f5f5d485de6e",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a726d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install opencv-python\n",
    "# %pip install mediapipe\n",
    "# %pip install tensorflow\n",
    "# %pip install matplotlib\n",
    "# %pip install scikit-learn\n",
    "# %pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e7b4676-e428-4ec4-b851-e26a33947ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import winsound\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score\n",
    "from scipy import stats\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de555518",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "<img src=\"https://www.researchgate.net/profile/Jungpil-Shin/publication/362351225/figure/fig3/AS:1183521695645696@1659184969422/Mediapipe-detects-33-nodes-of-the-human-pose.ppm\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2ff11962",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feed Input Source\n",
    "feed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0761b716",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "419e8a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # image is no longer writeable\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2d7ee0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Silhouette V1\n",
    "def sil_preprocess(frame,fgbg,blurValue,binThreshold):\n",
    "\n",
    "    #Background Subtraction\n",
    "    silhouette = fgbg.apply(frame)\n",
    "\n",
    "    #Noise Reduction\n",
    "    blur = cv2.medianBlur(silhouette, blurValue)\n",
    "    # blur = cv2.GaussianBlur(frame, (blurValue, blurValue), 0)\n",
    "\n",
    "    #Binarization\n",
    "    _, binary_mask = cv2.threshold(blur, binThreshold, 255, cv2.THRESH_BINARY)\n",
    "    \n",
    "    return binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f18f9267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\lance/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been loaded.\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model\n",
    "model = torch.hub.load('pytorch/vision:v0.6.0', 'deeplabv3_resnet101', pretrained=True)\n",
    "# Segment people only for the purpose of human silhouette extraction\n",
    "people_class = 15\n",
    "\n",
    "# Evaluate model\n",
    "model.eval()\n",
    "print (\"Model has been loaded.\")\n",
    "\n",
    "blur = torch.FloatTensor([[[[1.0, 2.0, 1.0],[2.0, 4.0, 2.0],[1.0, 2.0, 1.0]]]]) / 16.0\n",
    "\n",
    "# Use GPU if supported, for better performance\n",
    "if torch.cuda.is_available():\n",
    "\tmodel.to('cuda')\n",
    "\tblur = blur.to('cuda')\n",
    "\t\n",
    "# Apply preprocessing (normalization)\n",
    "preprocess = transforms.Compose([\n",
    "\ttransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7a2f476a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Silhouette V2\n",
    "def makeSegMask(img):\n",
    "    # Scale input frame\n",
    "\tframe_data = torch.FloatTensor( img ) / 255.0\n",
    "\n",
    "\tinput_tensor = preprocess(frame_data.permute(2, 0, 1))\n",
    "    \n",
    "    # Create mini-batch to be used by the model\n",
    "\tinput_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "    # Use GPU if supported, for better performance\n",
    "\tif torch.cuda.is_available():\n",
    "\t\tinput_batch = input_batch.to('cuda')\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\toutput = model(input_batch)['out'][0]\n",
    "\n",
    "\tsegmentation = output.argmax(0)\n",
    "\n",
    "\tbgOut = output[0:1][:][:]\n",
    "\ta = (1.0 - F.relu(torch.tanh(bgOut * 0.30 - 1.0))).pow(0.5) * 2.0\n",
    "\n",
    "\tpeople = segmentation.eq( torch.ones_like(segmentation).long().fill_(people_class) ).float()\n",
    "\n",
    "\tpeople.unsqueeze_(0).unsqueeze_(0)\n",
    "\t\n",
    "\tfor i in range(3):\n",
    "\t\tpeople = F.conv2d(people, blur, stride=1, padding=1)\n",
    "\n",
    "\t# Activation function to combine masks - F.hardtanh(a * b)\n",
    "\tcombined_mask = F.relu(F.hardtanh(a * (people.squeeze().pow(1.5)) ))\n",
    "\tcombined_mask = combined_mask.expand(1, 3, -1, -1)\n",
    "\n",
    "\tres = (combined_mask * 255.0).cpu().squeeze().byte().permute(1, 2, 0).numpy()\n",
    "\t\n",
    "\t_,thresh = cv2.threshold(res,127,255,cv2.THRESH_BINARY)\n",
    "\n",
    "\treturn thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ae90fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(feed)\n",
    "fgbg = cv2.createBackgroundSubtractorKNN()\n",
    "# fgbg = cv2.createBackgroundSubtractorKNN(history=200, dist2Threshold=30)\n",
    "with  mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened(): \n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        #Silhouette V1\n",
    "        blurValue = 13\n",
    "        binThreshold = 128\n",
    "\n",
    "        binary_mask = sil_preprocess(frame,fgbg,blurValue,binThreshold)\n",
    "        cv2.imshow('Gait ID (Silhouette V1)', binary_mask)\n",
    "\n",
    "        #Silhouette V2\n",
    "        sil = makeSegMask(frame)\n",
    "        cv2.imshow('Gait ID (Silhouette V2)', sil)\n",
    "\n",
    "        #RGB\n",
    "        image, results = mediapipe_detection(frame, holistic)\n",
    "        draw_landmarks(image, results)\n",
    "        cv2.imshow('Gait ID (RBG)', image)\n",
    "        \n",
    "        if cv2.waitKey(10) & 0xFF == 27:\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f477314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(results):\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    return pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8222829f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_test = extract_keypoints(results)\n",
    "# result_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b6ed6c",
   "metadata": {},
   "source": [
    "# Setup Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1af69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data, numpy arrays\n",
    "# DATA_PATH = os.path.join('Temp_Dataset') \n",
    "DATA_PATH = os.path.join('Custom_Gait_Dataset_V.4')\n",
    "EXPORTED = os.path.join('Exported')\n",
    "\n",
    "# Persons that we try to detect\n",
    "persons = np.array(['Hamza Tharwat','Lance Moheb','Mina Nady','Mohmed Youssef','Omar Amin','Romario Nagy','Seif Ibrahim','Ziad Diaa'])\n",
    "# persons = np.array(['Lance Moheb'])\n",
    "\n",
    "# 10 videos worth of data\n",
    "no_sequences = 10\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 90\n",
    "\n",
    "persons_len = len(persons)\n",
    "total_sequences = persons_len*no_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e9061",
   "metadata": {},
   "source": [
    "# Capturing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bafb492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for person in persons:\n",
    "#     for sequence in range(no_sequences):\n",
    "#         try: \n",
    "#             os.makedirs(os.path.join(EXPORTED, person, str(sequence)))\n",
    "#         except:\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cap = cv2.VideoCapture(feed)\n",
    "# # Set mediapipe model \n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "#     # NEW LOOP\n",
    "#     # Loop through persons\n",
    "#     for person in persons:\n",
    "#         for sequence in range(no_sequences):\n",
    "#             for frame_num in range(sequence_length):\n",
    "\n",
    "#                 # Read feed\n",
    "#                 ret, frame = cap.read()\n",
    "\n",
    "#                 # Make detections\n",
    "#                 image, results = mediapipe_detection(frame, holistic)\n",
    "\n",
    "#                 # Draw landmarks\n",
    "#                 draw_landmarks(image, results)\n",
    "                \n",
    "#                 # NEW Apply wait logic\n",
    "#                 if frame_num == 0: \n",
    "#                     cv2.putText(image, 'STARTING COLLECTION', (120,200), \n",
    "#                             cv2.FONT_HERSHEY_SIMPLEX, 1, (0,255, 0), 1, cv2.LINE_AA)\n",
    "#                     cv2.putText(image, 'Collecting frames for {} Video, frame Number #{}#'.format(person, sequence), (15,12), \n",
    "#                             cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "#                     winsound.Beep(1000,1000)\n",
    "#                     cv2.waitKey(5000)\n",
    "#                     winsound.Beep(1000,500)\n",
    "#                     winsound.Beep(1000,500)\n",
    "#                 else: \n",
    "#                     cv2.putText(image, 'Collecting frames for {} Video, frame Number #{}#'.format(person, sequence), (15,12), \n",
    "#                             cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "                    \n",
    "#                 # NEW Export keypoints\n",
    "#                 keypoints = extract_keypoints(results)\n",
    "#                 png_path = f\"{DATA_PATH}/{person}/{sequence}/{frame_num}\"\n",
    "#                 img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), 'RGB')\n",
    "#                 img.save(f'{png_path}.jpg')\n",
    "\n",
    "                \n",
    "#                 cv2.imshow('Gait ID', image)\n",
    "\n",
    "#                 # Break gracefully\n",
    "#                 if cv2.waitKey(10) & 0xFF == 27:\n",
    "#                     break\n",
    "#         if cv2.waitKey(10) & 0xFF == 27:\n",
    "#             continue\n",
    "#         else:\n",
    "#             break\n",
    "            \n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae804103",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a7c073",
   "metadata": {},
   "source": [
    "* A) Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b3be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_map = {label:num for num, label in enumerate(persons)}\n",
    "# print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22cd3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequences, labels = [], []\n",
    "# for person in persons:\n",
    "#     for sequence in np.array(os.listdir(os.path.join(DATA_PATH, person))).astype(int):\n",
    "#         window = []\n",
    "#         for frame_num in range(sequence_length):\n",
    "\n",
    "#             image_path = os.path.join(DATA_PATH, person, str(sequence), f\"{frame_num}.jpg\")\n",
    "#             print (image_path)\n",
    "#             res = cv2.imread(image_path)\n",
    "#             window.append(res)\n",
    "#         sequences.append(window)\n",
    "#         labels.append(label_map[person])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c625fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seqq=np.array(sequences)\n",
    "# print(seqq.shape)\n",
    "# show_test = cv2.cvtColor(seqq[1][40], cv2.COLOR_BGR2RGB)\n",
    "# plt.imshow(show_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2fd482",
   "metadata": {},
   "source": [
    "* B) Converting to Silhouette Using:\n",
    "    * Background Subtraction\n",
    "    * Noise Reduction\n",
    "    * Binarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f9580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exported = []\n",
    "# fgbg = cv2.createBackgroundSubtractorKNN(detectShadows=False)\n",
    "# # for sequence in  range(total_sequences):\n",
    "# for sequence in  range(4):\n",
    "#     for frame_num in range(sequence_length):\n",
    "\n",
    "#         frame = cv2.cvtColor(seqq[sequence][frame_num], cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#         sil = makeSegMask(frame)\n",
    "\n",
    "#         exported.append(sil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927d8830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(exported[140],cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e268f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming each person has 10 sequences, and each sequence has 90 frames\n",
    "# total_sequences = 10\n",
    "# sequence_length = 90\n",
    "\n",
    "# # Define the base directory where the images will be saved\n",
    "# base_dir = \"image_exports\"\n",
    "\n",
    "# # Create the base directory if it doesn't exist\n",
    "# if not os.path.exists(base_dir):\n",
    "#     os.makedirs(base_dir)\n",
    "\n",
    "# # Iterate through each person\n",
    "# for person_name in persons:\n",
    "#     person_dir = os.path.join(base_dir, person_name)\n",
    "#     # Create a directory for the person if it doesn't exist\n",
    "#     if not os.path.exists(person_dir):\n",
    "#         os.makedirs(person_dir)\n",
    "    \n",
    "#     # Iterate through each sequence for the person\n",
    "#     for sequence in range(total_sequences):\n",
    "#         sequence_dir = os.path.join(person_dir, f\"sequence_{sequence+1}\")\n",
    "#         # Create a directory for the sequence if it doesn't exist\n",
    "#         if not os.path.exists(sequence_dir):\n",
    "#             os.makedirs(sequence_dir)\n",
    "        \n",
    "#         # Iterate through each frame in the sequence\n",
    "#         for frame_num in range(sequence_length):\n",
    "#             # Generate the filename for the frame\n",
    "#             filename = f\"frame_{frame_num+1}.png\"\n",
    "#             filepath = os.path.join(sequence_dir, filename)\n",
    "            \n",
    "#             # Check if there are still images in the exported array\n",
    "#             if len(exported) > 0:\n",
    "#                 # Retrieve and save the next image from the exported array\n",
    "#                 cv2.imwrite(filepath, exported.pop(0))\n",
    "#             else:\n",
    "#                 # If there are no more images in the exported array, break the loop\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f617995f",
   "metadata": {},
   "source": [
    "* C) Removing Unwanted bLacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to crop silhouette images to the person\n",
    "# def crop_to_person(images):\n",
    "#     cropped_images = []\n",
    "#     for image in images:\n",
    "#         # Find contours\n",
    "#         contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "#         # Find the bounding box enclosing the contour\n",
    "#         if contours:\n",
    "#             x, y, w, h = cv2.boundingRect(contours[0])\n",
    "#             cropped_image = image[y:y+h, x:x+w]\n",
    "#         else:\n",
    "#             # If no contours found, return original image\n",
    "#             cropped_image = image\n",
    "        \n",
    "#         cropped_images.append(cropped_image)\n",
    "    \n",
    "#     return cropped_images\n",
    "# cropped_images = crop_to_person(exported)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8866b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(cropped_images[130],cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97208df7",
   "metadata": {},
   "source": [
    "* E) GEI Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04057f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_gei(images):\n",
    "#     # Convert images to numpy array\n",
    "#     images_array = np.array(images)\n",
    "\n",
    "#     # Compute GEI by averaging along the first axis (assuming images are stacked along the first axis)\n",
    "#     gei = np.mean(images_array, axis=0)\n",
    "\n",
    "#     return gei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6603174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for frame in range(sequence_length):\n",
    "#     temp = compute_gei(resized_images)\n",
    "#     plt.imshow(temp,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc825f3",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b34087",
   "metadata": {},
   "source": [
    "A) Splitting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceda2494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array(sequences)\n",
    "# y = to_categorical(labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# print(X_test.shape)\n",
    "# print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9427970c",
   "metadata": {},
   "source": [
    "B) Building CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a8e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_dir = os.path.join('Logs')\n",
    "# tb_callback = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n",
    "# from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebb7782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(60,132)))\n",
    "# model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "# model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(Dense(persons.shape[0], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b631369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Compilation\n",
    "# model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(X_train, y_train, epochs=200, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8028237",
   "metadata": {},
   "source": [
    "C) Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a203c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aae120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Evaluate on test data\")\n",
    "# results = model.evaluate(X_test, y_test, batch_size=128)\n",
    "# print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b088dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bccae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persons[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111484ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persons[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ee87f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('train4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4268cd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yhat = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2d91c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "# yhat = np.argmax(yhat, axis=1).tolist()\n",
    "\n",
    "# multilabel_confusion_matrix(ytrue, yhat)\n",
    "\n",
    "# accuracy_score(ytrue, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803529ec",
   "metadata": {},
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cbd420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = [(245,117,16),(245,117,16),(245,117,16),(245,117,16),(245,117,16),(245,117,16),(245,117,16),(245,117,16),(245,117,16),(245,117,16),(245,117,16),(245,117,16)]\n",
    "# def prob_viz(res, actions, input_frame, colors):\n",
    "#     output_frame = input_frame.copy()\n",
    "#     for num, prob in enumerate(res):\n",
    "#         cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "#         cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 1, cv2.LINE_AA)\n",
    "        \n",
    "#     return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3639b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1. New detection variables\n",
    "# sequence = []\n",
    "# sentence = []\n",
    "# predictions = []\n",
    "# threshold = 0.3\n",
    "\n",
    "# cap = cv2.VideoCapture(feed)\n",
    "# # Set mediapipe model\n",
    "# with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "#     while cap.isOpened():\n",
    "\n",
    "#         # Read feed\n",
    "#         ret, frame = cap.read()\n",
    "\n",
    "#         # Make detections\n",
    "#         image, results = mediapipe_detection(frame, holistic)\n",
    "#         print(results)\n",
    "        \n",
    "#         # Draw landmarks\n",
    "#         draw_landmarks(image, results)\n",
    "        \n",
    "#         # 2. Prediction logic\n",
    "#         keypoints = extract_keypoints(results)\n",
    "#         sequence.append(keypoints)\n",
    "#         sequence = sequence[-60:]\n",
    "        \n",
    "#         if len(sequence) == 60:\n",
    "#             res = model.predict(np.expand_dims(sequence, axis=0))[0]\n",
    "#             print(persons[np.argmax(res)])\n",
    "#             predictions.append(np.argmax(res))\n",
    "            \n",
    "            \n",
    "#         #3. Viz logic\n",
    "#             if np.unique(predictions[-10:])[0]==np.argmax(res): \n",
    "#                 if res[np.argmax(res)] > threshold: \n",
    "                    \n",
    "#                     if len(sentence) > 0: \n",
    "#                         if persons[np.argmax(res)] != sentence[-1]:\n",
    "#                             sentence.append(persons[np.argmax(res)])\n",
    "#                     else:\n",
    "#                         sentence.append(persons[np.argmax(res)])\n",
    "\n",
    "#             if len(sentence) > 5: \n",
    "#                 sentence = sentence[-5:]\n",
    "\n",
    "#             # Viz probabilities\n",
    "#             image = prob_viz(res, persons, image, colors)\n",
    "            \n",
    "#         cv2.rectangle(image, (0,0), (640, 40), (245, 117, 16), -1)\n",
    "                \n",
    "#         cv2.putText(image, ' '.join(sentence), (3,30), \n",
    "#                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 1, cv2.LINE_AA)\n",
    "        \n",
    "#         # Show to screen\n",
    "#         cv2.imshow('Gait ID', image)\n",
    "\n",
    "#         # Break gracefully\n",
    "#         if cv2.waitKey(10) & 0xFF == 27:\n",
    "#             break\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
